{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms, utils\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ch = 3\n",
    "hidden_ch = 128\n",
    "out_dim = 10\n",
    "device_id = 0\n",
    "device = 'cpu' if device_id == -1 else f'cuda:{device_id}'\n",
    "n_epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e267ea6df0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(trainset.data[228])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 5000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset, valset = random_split(trainset, [45000, 5000])\n",
    "len(trainset), len(valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_ch, output_ch, identity_downsample=None, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_ch, output_ch, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(output_ch)\n",
    "        self.conv2 = nn.Conv2d(output_ch, output_ch, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(output_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_channels, num_classes):\n",
    "        \n",
    "        super(ResNet18, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        #resnet layers\n",
    "        self.layer1 = self.__make_layer(64, 64, stride=1)\n",
    "        self.layer2 = self.__make_layer(64, 128, stride=2)\n",
    "        self.layer3 = self.__make_layer(128, 256, stride=2)\n",
    "        self.layer4 = self.__make_layer(256, 512, stride=2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def __make_layer(self, in_channels, out_channels, stride):\n",
    "        \n",
    "        identity_downsample = None\n",
    "        if stride != 1:\n",
    "            identity_downsample = self.identity_downsample(in_channels, out_channels)\n",
    "            \n",
    "        return nn.Sequential(\n",
    "            ResidualBlock(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride), \n",
    "            ResidualBlock(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x \n",
    "    \n",
    "    def identity_downsample(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18(3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12555018"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count trainable parameters of the model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#move the model to the device\n",
    "model.to(device)\n",
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define everything we need for training\n",
    "epochs = 5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax0 = fig.add_subplot(121, title=\"loss\")\n",
    "ax1 = fig.add_subplot(122, title=\"top1err\")\n",
    "x_epoch = []\n",
    "y_loss = {}  # loss history\n",
    "y_loss['train'] = []\n",
    "y_loss['val'] = []\n",
    "y_err = {}\n",
    "y_err['train'] = []\n",
    "y_err['val'] = []\n",
    "\n",
    "\n",
    "def draw_curve(current_epoch):\n",
    "    x_epoch.append(current_epoch)\n",
    "    ax0.plot(x_epoch, y_loss['train'], 'bo-', label='train')\n",
    "    ax0.plot(x_epoch, y_loss['val'], 'ro-', label='val')\n",
    "    ax1.plot(x_epoch, y_err['train'], 'bo-', label='train')\n",
    "    ax1.plot(x_epoch, y_err['val'], 'ro-', label='val')\n",
    "    if current_epoch == 0:\n",
    "        ax0.legend()\n",
    "        ax1.legend()\n",
    "    fig.savefig(os.path.join('./lossGraphs', 'train.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=50):\n",
    "    \n",
    "    since = time.time()\n",
    "    val_acc_history = []\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']: # Each epoch has a training and validation phase\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]: # Iterate over data\n",
    "                \n",
    "                #inputs = transforms.functional.resize(inputs, (112, 112))\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad() # Zero the parameter gradients\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'): # Forward. Track history if only in train\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train': # Backward + optimize only if in training phase\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            if phase == 'val': # Adjust learning rate based on val loss\n",
    "                lr_scheduler.step(epoch_loss)\n",
    "                \n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            y_loss[phase].append(epoch_loss)\n",
    "            y_err[phase].append(1.0 - epoch_acc)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "            if phase == 'val':\n",
    "                draw_curve(epoch)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/4\n",
      "----------\n",
      "train Loss: 1.2327 Acc: 0.5564\n",
      "val Loss: 0.8286 Acc: 0.7062\n",
      "\n",
      "Epoch 1/4\n",
      "----------\n",
      "train Loss: 0.7956 Acc: 0.7205\n",
      "val Loss: 0.5952 Acc: 0.8012\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n",
      "train Loss: 0.5534 Acc: 0.8068\n",
      "val Loss: 0.3734 Acc: 0.8736\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n",
      "train Loss: 0.3523 Acc: 0.8767\n",
      "val Loss: 0.2312 Acc: 0.9232\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n",
      "train Loss: 0.2330 Acc: 0.9187\n",
      "val Loss: 0.1695 Acc: 0.9452\n",
      "\n",
      "Training complete in 3m 49s\n",
      "Best val Acc: 0.945200\n"
     ]
    }
   ],
   "source": [
    "model, _ = train_model(model, {\"train\": trainloader, \"val\": valloader}, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.2706,  0.2471,  0.2471,  ...,  0.1843,  0.1843,  0.1765],\n",
      "          [ 0.3098,  0.2863,  0.2863,  ...,  0.2549,  0.2471,  0.2392],\n",
      "          [ 0.3412,  0.3176,  0.3176,  ...,  0.3098,  0.3098,  0.3020],\n",
      "          ...,\n",
      "          [ 0.5529,  0.5451,  0.5451,  ...,  0.5451,  0.5373,  0.5294],\n",
      "          [ 0.5451,  0.5294,  0.5373,  ...,  0.5373,  0.5216,  0.5137],\n",
      "          [ 0.5451,  0.5294,  0.5373,  ...,  0.5373,  0.5216,  0.5137]],\n",
      "\n",
      "         [[ 0.5059,  0.4824,  0.4824,  ...,  0.4667,  0.4667,  0.4588],\n",
      "          [ 0.5451,  0.5216,  0.5216,  ...,  0.5137,  0.5137,  0.5059],\n",
      "          [ 0.5765,  0.5529,  0.5529,  ...,  0.5529,  0.5529,  0.5451],\n",
      "          ...,\n",
      "          [ 0.7647,  0.7490,  0.7490,  ...,  0.7412,  0.7412,  0.7333],\n",
      "          [ 0.7490,  0.7333,  0.7412,  ...,  0.7333,  0.7255,  0.7176],\n",
      "          [ 0.7490,  0.7333,  0.7412,  ...,  0.7333,  0.7255,  0.7176]],\n",
      "\n",
      "         [[ 0.5922,  0.5686,  0.5686,  ...,  0.5765,  0.5765,  0.5686],\n",
      "          [ 0.6314,  0.6078,  0.6078,  ...,  0.6314,  0.6314,  0.6235],\n",
      "          [ 0.6627,  0.6392,  0.6392,  ...,  0.6706,  0.6706,  0.6627],\n",
      "          ...,\n",
      "          [ 0.8667,  0.8510,  0.8510,  ...,  0.8510,  0.8431,  0.8353],\n",
      "          [ 0.8510,  0.8353,  0.8431,  ...,  0.8353,  0.8275,  0.8196],\n",
      "          [ 0.8510,  0.8353,  0.8431,  ...,  0.8353,  0.8275,  0.8196]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3569,  0.4039,  0.2706,  ...,  0.2941,  0.3333,  0.4667],\n",
      "          [ 0.3412,  0.3961,  0.3098,  ...,  0.3412,  0.3176,  0.3569],\n",
      "          [ 0.4588,  0.4588,  0.4353,  ...,  0.2941,  0.3569,  0.4275],\n",
      "          ...,\n",
      "          [ 0.4118,  0.5451,  0.6314,  ...,  0.4510,  0.4510,  0.4275],\n",
      "          [ 0.4431,  0.4980,  0.5373,  ...,  0.4118,  0.3176,  0.2941],\n",
      "          [ 0.4588,  0.4824,  0.4510,  ...,  0.3020,  0.2784,  0.3020]],\n",
      "\n",
      "         [[ 0.0824,  0.1294,  0.0039,  ..., -0.0353,  0.0039,  0.1373],\n",
      "          [ 0.0510,  0.1059,  0.0275,  ...,  0.0039, -0.0196,  0.0275],\n",
      "          [ 0.1451,  0.1529,  0.1294,  ..., -0.0431,  0.0196,  0.0980],\n",
      "          ...,\n",
      "          [ 0.0588,  0.1922,  0.2941,  ...,  0.1373,  0.1373,  0.1059],\n",
      "          [ 0.0902,  0.1216,  0.1686,  ...,  0.0745, -0.0118, -0.0353],\n",
      "          [ 0.1059,  0.0980,  0.0745,  ..., -0.0353, -0.0588, -0.0275]],\n",
      "\n",
      "         [[-0.2157, -0.1843, -0.3412,  ..., -0.3961, -0.3490, -0.2078],\n",
      "          [-0.2549, -0.2078, -0.3020,  ..., -0.3333, -0.3882, -0.3490],\n",
      "          [-0.1765, -0.1529, -0.1765,  ..., -0.3725, -0.3412, -0.2784],\n",
      "          ...,\n",
      "          [-0.3020, -0.1843, -0.1137,  ..., -0.2627, -0.2627, -0.2863],\n",
      "          [-0.2549, -0.2549, -0.2157,  ..., -0.2941, -0.3804, -0.4196],\n",
      "          [-0.2392, -0.2784, -0.2941,  ..., -0.3882, -0.4118, -0.4039]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8275,  0.8039,  0.8039,  ...,  0.8118,  0.8118,  0.8118],\n",
      "          [ 0.8431,  0.8196,  0.8196,  ...,  0.8275,  0.8275,  0.8275],\n",
      "          [ 0.8431,  0.8196,  0.8196,  ...,  0.8275,  0.8275,  0.8275],\n",
      "          ...,\n",
      "          [ 0.8667,  0.8431,  0.8510,  ...,  0.8667,  0.8667,  0.8510],\n",
      "          [ 0.8353,  0.8196,  0.8275,  ...,  0.8431,  0.8353,  0.8275],\n",
      "          [ 0.8118,  0.7882,  0.7882,  ...,  0.7961,  0.7961,  0.7882]],\n",
      "\n",
      "         [[ 0.9294,  0.8980,  0.9059,  ...,  0.9137,  0.9137,  0.9137],\n",
      "          [ 0.9451,  0.9216,  0.9216,  ...,  0.9294,  0.9294,  0.9294],\n",
      "          [ 0.9451,  0.9216,  0.9216,  ...,  0.9294,  0.9294,  0.9294],\n",
      "          ...,\n",
      "          [ 1.0000,  0.9843,  0.9922,  ...,  0.9765,  0.9765,  0.9608],\n",
      "          [ 0.9843,  0.9686,  0.9765,  ...,  0.9608,  0.9608,  0.9451],\n",
      "          [ 0.9529,  0.9294,  0.9294,  ...,  0.9294,  0.9294,  0.9216]],\n",
      "\n",
      "         [[ 1.0000,  0.9686,  0.9765,  ...,  0.9608,  0.9608,  0.9608],\n",
      "          [ 1.0000,  0.9922,  0.9922,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          [ 1.0000,  0.9922,  0.9922,  ...,  0.9765,  0.9765,  0.9765],\n",
      "          ...,\n",
      "          [ 1.0000,  0.9765,  0.9765,  ...,  1.0000,  0.9922,  0.9843],\n",
      "          [ 0.9843,  0.9608,  0.9686,  ...,  0.9922,  0.9843,  0.9765],\n",
      "          [ 0.9765,  0.9529,  0.9529,  ...,  0.9608,  0.9608,  0.9529]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.7490, -0.6863, -0.6941,  ..., -0.8353, -0.8980, -0.8353],\n",
      "          [-0.8510, -0.7412, -0.7176,  ..., -0.8118, -0.8745, -0.7255],\n",
      "          [-0.8039, -0.6863, -0.4980,  ..., -0.8902, -0.9294, -0.7098],\n",
      "          ...,\n",
      "          [-0.1843,  0.2784,  0.1373,  ...,  0.1765, -0.3804, -0.6863],\n",
      "          [-0.2784,  0.3255,  0.2549,  ..., -0.1608, -0.6235, -0.6784],\n",
      "          [-0.3333,  0.0353,  0.3255,  ..., -0.4510, -0.6314, -0.5922]],\n",
      "\n",
      "         [[-0.7804, -0.7333, -0.7725,  ..., -0.8431, -0.8980, -0.8353],\n",
      "          [-0.9059, -0.8588, -0.8745,  ..., -0.8275, -0.8667, -0.7176],\n",
      "          [-0.8902, -0.8588, -0.6941,  ..., -0.9294, -0.9294, -0.7020],\n",
      "          ...,\n",
      "          [-0.5608, -0.4588, -0.4353,  ..., -0.4039, -0.7098, -0.8275],\n",
      "          [-0.6471, -0.3255, -0.4431,  ..., -0.6078, -0.8275, -0.8039],\n",
      "          [-0.6941, -0.4510, -0.3098,  ..., -0.7255, -0.7725, -0.7490]],\n",
      "\n",
      "         [[-0.8039, -0.7647, -0.8039,  ..., -0.8510, -0.8980, -0.8275],\n",
      "          [-0.9294, -0.8824, -0.9059,  ..., -0.8431, -0.8745, -0.7176],\n",
      "          [-0.9137, -0.8902, -0.7490,  ..., -0.9451, -0.9451, -0.7098],\n",
      "          ...,\n",
      "          [-0.6471, -0.5373, -0.5294,  ..., -0.4902, -0.7569, -0.8510],\n",
      "          [-0.7255, -0.4353, -0.5451,  ..., -0.6235, -0.8353, -0.8196],\n",
      "          [-0.7961, -0.5294, -0.4118,  ..., -0.7176, -0.7725, -0.7725]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1373,  0.0353,  0.1608,  ...,  0.2549,  0.2549,  0.3020],\n",
      "          [ 0.2314,  0.0824,  0.1686,  ...,  0.1137,  0.3020,  0.4275],\n",
      "          [ 0.3569,  0.1608,  0.1608,  ...,  0.2078,  0.2078,  0.2392],\n",
      "          ...,\n",
      "          [ 0.1765,  0.3490,  0.2863,  ...,  0.3098,  0.2627,  0.1137],\n",
      "          [ 0.1216,  0.1529,  0.0824,  ...,  0.2863,  0.3255,  0.3098],\n",
      "          [ 0.0667,  0.1294,  0.2314,  ...,  0.3412,  0.3725,  0.3804]],\n",
      "\n",
      "         [[ 0.0431, -0.0353,  0.0980,  ...,  0.2235,  0.2314,  0.2863],\n",
      "          [ 0.1216, -0.0039,  0.1137,  ...,  0.0353,  0.2314,  0.3804],\n",
      "          [ 0.2314,  0.0588,  0.0980,  ...,  0.1216,  0.1373,  0.2000],\n",
      "          ...,\n",
      "          [ 0.1059,  0.2941,  0.2392,  ...,  0.1765,  0.1216, -0.0353],\n",
      "          [ 0.0667,  0.1059,  0.0353,  ...,  0.1294,  0.1529,  0.1216],\n",
      "          [ 0.0275,  0.0588,  0.1529,  ...,  0.1686,  0.1765,  0.1451]],\n",
      "\n",
      "         [[ 0.0275, -0.0510,  0.0353,  ...,  0.1686,  0.1765,  0.2471],\n",
      "          [ 0.0588, -0.0667,  0.0039,  ..., -0.0510,  0.1451,  0.3176],\n",
      "          [ 0.1608, -0.0118, -0.0118,  ...,  0.0275,  0.0510,  0.1451],\n",
      "          ...,\n",
      "          [ 0.0510,  0.2235,  0.1686,  ...,  0.0196, -0.0431, -0.1686],\n",
      "          [ 0.0039,  0.0431, -0.0118,  ..., -0.0196,  0.0118,  0.0196],\n",
      "          [-0.0196,  0.0196,  0.1137,  ...,  0.0588,  0.0745,  0.0667]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3725,  0.3098,  0.2863,  ...,  0.2000,  0.3020,  0.2784],\n",
      "          [ 0.3569,  0.3098,  0.2863,  ...,  0.5059,  0.4196,  0.3176],\n",
      "          [ 0.3333,  0.2941,  0.2941,  ...,  0.5451,  0.6157,  0.5843],\n",
      "          ...,\n",
      "          [ 0.1059, -0.3569, -0.8510,  ..., -0.9216, -0.8588, -0.7961],\n",
      "          [ 0.1608, -0.3961, -0.9843,  ..., -0.7255, -0.6706, -0.6471],\n",
      "          [ 0.1294, -0.0431, -0.4588,  ..., -0.5765, -0.5451, -0.5294]],\n",
      "\n",
      "         [[ 0.7490,  0.6784,  0.6627,  ...,  0.6000,  0.7569,  0.7098],\n",
      "          [ 0.7255,  0.6706,  0.6549,  ...,  0.6706,  0.7098,  0.6941],\n",
      "          [ 0.7098,  0.6627,  0.6627,  ...,  0.2784,  0.5529,  0.7569],\n",
      "          ...,\n",
      "          [ 0.3333, -0.0824, -0.5373,  ..., -0.7804, -0.6863, -0.6078],\n",
      "          [ 0.3961, -0.1294, -0.6941,  ..., -0.6471, -0.5843, -0.5529],\n",
      "          [ 0.3647,  0.2000, -0.2235,  ..., -0.5686, -0.5294, -0.5137]],\n",
      "\n",
      "         [[ 0.4745,  0.4118,  0.3961,  ...,  0.3569,  0.4980,  0.4431],\n",
      "          [ 0.4588,  0.4118,  0.3882,  ...,  0.4824,  0.4667,  0.3961],\n",
      "          [ 0.4353,  0.3961,  0.3961,  ...,  0.1373,  0.3569,  0.5059],\n",
      "          ...,\n",
      "          [-0.1765, -0.5059, -0.8275,  ..., -0.9451, -0.9373, -0.9451],\n",
      "          [-0.1294, -0.5686, -0.9843,  ..., -0.9451, -0.9373, -0.9765],\n",
      "          [-0.1843, -0.2941, -0.5843,  ..., -0.9843, -0.9843, -1.0000]]]])\n",
      "-----------------------------\n",
      "tensor([[[[ 0.2706,  0.2706,  0.2655,  ...,  0.1782,  0.1765,  0.1765],\n",
      "          [ 0.2706,  0.2706,  0.2655,  ...,  0.1782,  0.1765,  0.1765],\n",
      "          [ 0.2790,  0.2790,  0.2739,  ...,  0.1916,  0.1899,  0.1899],\n",
      "          ...,\n",
      "          [ 0.5451,  0.5451,  0.5417,  ...,  0.5154,  0.5137,  0.5137],\n",
      "          [ 0.5451,  0.5451,  0.5417,  ...,  0.5154,  0.5137,  0.5137],\n",
      "          [ 0.5451,  0.5451,  0.5417,  ...,  0.5154,  0.5137,  0.5137]],\n",
      "\n",
      "         [[ 0.5059,  0.5059,  0.5008,  ...,  0.4605,  0.4588,  0.4588],\n",
      "          [ 0.5059,  0.5059,  0.5008,  ...,  0.4605,  0.4588,  0.4588],\n",
      "          [ 0.5143,  0.5143,  0.5092,  ...,  0.4706,  0.4689,  0.4689],\n",
      "          ...,\n",
      "          [ 0.7490,  0.7490,  0.7457,  ...,  0.7193,  0.7176,  0.7176],\n",
      "          [ 0.7490,  0.7490,  0.7457,  ...,  0.7193,  0.7176,  0.7176],\n",
      "          [ 0.7490,  0.7490,  0.7457,  ...,  0.7193,  0.7176,  0.7176]],\n",
      "\n",
      "         [[ 0.5922,  0.5922,  0.5871,  ...,  0.5703,  0.5686,  0.5686],\n",
      "          [ 0.5922,  0.5922,  0.5871,  ...,  0.5703,  0.5686,  0.5686],\n",
      "          [ 0.6006,  0.6006,  0.5955,  ...,  0.5821,  0.5804,  0.5804],\n",
      "          ...,\n",
      "          [ 0.8510,  0.8510,  0.8476,  ...,  0.8213,  0.8196,  0.8196],\n",
      "          [ 0.8510,  0.8510,  0.8476,  ...,  0.8213,  0.8196,  0.8196],\n",
      "          [ 0.8510,  0.8510,  0.8476,  ...,  0.8213,  0.8196,  0.8196]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3569,  0.3569,  0.3669,  ...,  0.4381,  0.4667,  0.4667],\n",
      "          [ 0.3569,  0.3569,  0.3669,  ...,  0.4381,  0.4667,  0.4667],\n",
      "          [ 0.3535,  0.3535,  0.3639,  ...,  0.4189,  0.4431,  0.4431],\n",
      "          ...,\n",
      "          [ 0.4555,  0.4555,  0.4619,  ...,  0.2974,  0.3003,  0.3003],\n",
      "          [ 0.4588,  0.4588,  0.4639,  ...,  0.2969,  0.3020,  0.3020],\n",
      "          [ 0.4588,  0.4588,  0.4639,  ...,  0.2969,  0.3020,  0.3020]],\n",
      "\n",
      "         [[ 0.0824,  0.0824,  0.0924,  ...,  0.1087,  0.1373,  0.1373],\n",
      "          [ 0.0824,  0.0824,  0.0924,  ...,  0.1087,  0.1373,  0.1373],\n",
      "          [ 0.0756,  0.0756,  0.0861,  ...,  0.0891,  0.1137,  0.1137],\n",
      "          ...,\n",
      "          [ 0.1025,  0.1025,  0.1026,  ..., -0.0333, -0.0291, -0.0291],\n",
      "          [ 0.1059,  0.1059,  0.1042,  ..., -0.0342, -0.0275, -0.0275],\n",
      "          [ 0.1059,  0.1059,  0.1042,  ..., -0.0342, -0.0275, -0.0275]],\n",
      "\n",
      "         [[-0.2157, -0.2157, -0.2090,  ..., -0.2381, -0.2078, -0.2078],\n",
      "          [-0.2157, -0.2157, -0.2090,  ..., -0.2381, -0.2078, -0.2078],\n",
      "          [-0.2241, -0.2241, -0.2166,  ..., -0.2637, -0.2381, -0.2381],\n",
      "          ...,\n",
      "          [-0.2426, -0.2426, -0.2492,  ..., -0.4068, -0.4073, -0.4073],\n",
      "          [-0.2392, -0.2392, -0.2476,  ..., -0.4056, -0.4039, -0.4039],\n",
      "          [-0.2392, -0.2392, -0.2476,  ..., -0.4056, -0.4039, -0.4039]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8275,  0.8275,  0.8224,  ...,  0.8118,  0.8118,  0.8118],\n",
      "          [ 0.8275,  0.8275,  0.8224,  ...,  0.8118,  0.8118,  0.8118],\n",
      "          [ 0.8308,  0.8308,  0.8258,  ...,  0.8151,  0.8151,  0.8151],\n",
      "          ...,\n",
      "          [ 0.8168,  0.8168,  0.8121,  ...,  0.7983,  0.7966,  0.7966],\n",
      "          [ 0.8118,  0.8118,  0.8067,  ...,  0.7899,  0.7882,  0.7882],\n",
      "          [ 0.8118,  0.8118,  0.8067,  ...,  0.7899,  0.7882,  0.7882]],\n",
      "\n",
      "         [[ 0.9294,  0.9294,  0.9227,  ...,  0.9137,  0.9137,  0.9137],\n",
      "          [ 0.9294,  0.9294,  0.9227,  ...,  0.9137,  0.9137,  0.9137],\n",
      "          [ 0.9328,  0.9328,  0.9264,  ...,  0.9171,  0.9171,  0.9171],\n",
      "          ...,\n",
      "          [ 0.9597,  0.9597,  0.9550,  ...,  0.9287,  0.9266,  0.9266],\n",
      "          [ 0.9529,  0.9529,  0.9479,  ...,  0.9232,  0.9216,  0.9216],\n",
      "          [ 0.9529,  0.9529,  0.9479,  ...,  0.9232,  0.9216,  0.9216]],\n",
      "\n",
      "         [[ 1.0000,  1.0000,  0.9933,  ...,  0.9608,  0.9608,  0.9608],\n",
      "          [ 1.0000,  1.0000,  0.9933,  ...,  0.9608,  0.9608,  0.9608],\n",
      "          [ 1.0000,  1.0000,  0.9944,  ...,  0.9641,  0.9641,  0.9641],\n",
      "          ...,\n",
      "          [ 0.9782,  0.9782,  0.9731,  ...,  0.9597,  0.9580,  0.9580],\n",
      "          [ 0.9765,  0.9765,  0.9714,  ...,  0.9546,  0.9529,  0.9529],\n",
      "          [ 0.9765,  0.9765,  0.9714,  ...,  0.9546,  0.9529,  0.9529]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.7490, -0.7490, -0.7356,  ..., -0.8487, -0.8353, -0.8353],\n",
      "          [-0.7490, -0.7490, -0.7356,  ..., -0.8487, -0.8353, -0.8353],\n",
      "          [-0.7709, -0.7709, -0.7553,  ..., -0.8292, -0.8118, -0.8118],\n",
      "          ...,\n",
      "          [-0.3216, -0.3216, -0.2318,  ..., -0.6147, -0.6106, -0.6106],\n",
      "          [-0.3333, -0.3333, -0.2543,  ..., -0.6006, -0.5922, -0.5922],\n",
      "          [-0.3333, -0.3333, -0.2543,  ..., -0.6006, -0.5922, -0.5922]],\n",
      "\n",
      "         [[-0.7804, -0.7804, -0.7703,  ..., -0.8487, -0.8353, -0.8353],\n",
      "          [-0.7804, -0.7804, -0.7703,  ..., -0.8487, -0.8353, -0.8353],\n",
      "          [-0.8073, -0.8073, -0.7972,  ..., -0.8275, -0.8101, -0.8101],\n",
      "          ...,\n",
      "          [-0.6840, -0.6840, -0.6283,  ..., -0.7658, -0.7608, -0.7608],\n",
      "          [-0.6941, -0.6941, -0.6420,  ..., -0.7541, -0.7490, -0.7490],\n",
      "          [-0.6941, -0.6941, -0.6420,  ..., -0.7541, -0.7490, -0.7490]],\n",
      "\n",
      "         [[-0.8039, -0.8039, -0.7955,  ..., -0.8426, -0.8275, -0.8275],\n",
      "          [-0.8039, -0.8039, -0.7955,  ..., -0.8426, -0.8275, -0.8275],\n",
      "          [-0.8308, -0.8308, -0.8220,  ..., -0.8230, -0.8039, -0.8039],\n",
      "          ...,\n",
      "          [-0.7810, -0.7810, -0.7227,  ..., -0.7834, -0.7826, -0.7826],\n",
      "          [-0.7961, -0.7961, -0.7389,  ..., -0.7725, -0.7725, -0.7725],\n",
      "          [-0.7961, -0.7961, -0.7389,  ..., -0.7725, -0.7725, -0.7725]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1373,  0.1373,  0.1154,  ...,  0.2919,  0.3020,  0.3020],\n",
      "          [ 0.1373,  0.1373,  0.1154,  ...,  0.2919,  0.3020,  0.3020],\n",
      "          [ 0.1574,  0.1574,  0.1334,  ...,  0.3152,  0.3289,  0.3289],\n",
      "          ...,\n",
      "          [ 0.0784,  0.0784,  0.0904,  ...,  0.3647,  0.3653,  0.3653],\n",
      "          [ 0.0667,  0.0667,  0.0801,  ...,  0.3787,  0.3804,  0.3804],\n",
      "          [ 0.0667,  0.0667,  0.0801,  ...,  0.3787,  0.3804,  0.3804]],\n",
      "\n",
      "         [[ 0.0431,  0.0431,  0.0263,  ...,  0.2745,  0.2863,  0.2863],\n",
      "          [ 0.0431,  0.0431,  0.0263,  ...,  0.2745,  0.2863,  0.2863],\n",
      "          [ 0.0599,  0.0599,  0.0410,  ...,  0.2904,  0.3064,  0.3064],\n",
      "          ...,\n",
      "          [ 0.0359,  0.0359,  0.0429,  ...,  0.1468,  0.1401,  0.1401],\n",
      "          [ 0.0275,  0.0275,  0.0342,  ...,  0.1518,  0.1451,  0.1451],\n",
      "          [ 0.0275,  0.0275,  0.0342,  ...,  0.1518,  0.1451,  0.1451]],\n",
      "\n",
      "         [[ 0.0275,  0.0275,  0.0106,  ...,  0.2319,  0.2471,  0.2471],\n",
      "          [ 0.0275,  0.0275,  0.0106,  ...,  0.2319,  0.2471,  0.2471],\n",
      "          [ 0.0342,  0.0342,  0.0152,  ...,  0.2424,  0.2622,  0.2622],\n",
      "          ...,\n",
      "          [-0.0146, -0.0146, -0.0062,  ...,  0.0575,  0.0566,  0.0566],\n",
      "          [-0.0196, -0.0196, -0.0112,  ...,  0.0683,  0.0667,  0.0667],\n",
      "          [-0.0196, -0.0196, -0.0112,  ...,  0.0683,  0.0667,  0.0667]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3725,  0.3725,  0.3591,  ...,  0.2835,  0.2784,  0.2784],\n",
      "          [ 0.3725,  0.3725,  0.3591,  ...,  0.2835,  0.2784,  0.2784],\n",
      "          [ 0.3692,  0.3692,  0.3565,  ...,  0.2955,  0.2868,  0.2868],\n",
      "          ...,\n",
      "          [ 0.1361,  0.1361,  0.0815,  ..., -0.5583, -0.5546, -0.5546],\n",
      "          [ 0.1294,  0.1294,  0.0924,  ..., -0.5328, -0.5294, -0.5294],\n",
      "          [ 0.1294,  0.1294,  0.0924,  ..., -0.5328, -0.5294, -0.5294]],\n",
      "\n",
      "         [[ 0.7490,  0.7490,  0.7339,  ...,  0.7199,  0.7098,  0.7098],\n",
      "          [ 0.7490,  0.7490,  0.7339,  ...,  0.7199,  0.7098,  0.7098],\n",
      "          [ 0.7440,  0.7440,  0.7296,  ...,  0.7151,  0.7064,  0.7064],\n",
      "          ...,\n",
      "          [ 0.3714,  0.3714,  0.3196,  ..., -0.5262, -0.5221, -0.5221],\n",
      "          [ 0.3647,  0.3647,  0.3294,  ..., -0.5171, -0.5137, -0.5137],\n",
      "          [ 0.3647,  0.3647,  0.3294,  ..., -0.5171, -0.5137, -0.5137]],\n",
      "\n",
      "         [[ 0.4745,  0.4745,  0.4611,  ...,  0.4549,  0.4431,  0.4431],\n",
      "          [ 0.4745,  0.4745,  0.4611,  ...,  0.4549,  0.4431,  0.4431],\n",
      "          [ 0.4711,  0.4711,  0.4584,  ...,  0.4455,  0.4331,  0.4331],\n",
      "          ...,\n",
      "          [-0.1725, -0.1725, -0.2112,  ..., -0.9905, -0.9950, -0.9950],\n",
      "          [-0.1843, -0.1843, -0.2078,  ..., -0.9966, -1.0000, -1.0000],\n",
      "          [-0.1843, -0.1843, -0.2078,  ..., -0.9966, -1.0000, -1.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax0 = fig.add_subplot(121, title=\"loss\")\n",
    "ax1 = fig.add_subplot(122, title=\"top1err\")\n",
    "\n",
    "def draw_curve(current_epoch):\n",
    "    x_epoch.append(current_epoch)\n",
    "    ax0.plot(x_epoch, y_loss['train'], 'bo-', label='train')\n",
    "    ax0.plot(x_epoch, y_loss['val'], 'ro-', label='val')\n",
    "    ax1.plot(x_epoch, y_err['train'], 'bo-', label='train')\n",
    "    ax1.plot(x_epoch, y_err['val'], 'ro-', label='val')\n",
    "    if current_epoch == 0:\n",
    "        ax0.legend()\n",
    "        ax1.legend()\n",
    "    fig.savefig(os.path.join('./lossGraphs', 'train.jpg'))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "    print('-' * 10)\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train(True)  # Set model to training mode\n",
    "        else:\n",
    "            model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        count = 0\n",
    "        # Iterate over data.\n",
    "        for data in dataloaders[phase]:\n",
    "            if count > 10:\n",
    "                break\n",
    "\n",
    "            count = count + 1\n",
    "            # get a batch of inputs\n",
    "            inputs, labels = data\n",
    "            now_batch_size, c, h, w = inputs.shape\n",
    "            if now_batch_size < batchsize:  # skip the last batch\n",
    "                continue\n",
    "            # print(inputs.shape)\n",
    "            # wrap them in Variable, if gpu is used, we transform the data to cuda.\n",
    "            if use_gpu:\n",
    "                inputs = Variable(inputs.cuda())\n",
    "                labels = Variable(labels.cuda())\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # -------- forward --------\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            del inputs\n",
    "\n",
    "            # -------- backward + optimize --------\n",
    "            # only if in training phase\n",
    "            if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * now_batch_size\n",
    "            del loss\n",
    "            running_corrects += float(torch.sum(preds == labels.data))\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "        y_loss[phase].append(epoch_loss)\n",
    "        y_err[phase].append(1.0 - epoch_acc)\n",
    "\n",
    "        # deep copy the model\n",
    "        if phase == 'val':\n",
    "            draw_curve(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d07921fcac9efc71e32baa62f54cc7cc7703180b766de90eef3b067ead514a11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
